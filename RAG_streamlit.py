import os  # íŒŒì¼ ì‘ì—…ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import tempfile  # ì„ì‹œ íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st  # Streamlitì„ ì‚¬ìš©í•´ ì›¹ ì•± êµ¬ì¶•
from langchain.chat_models import ChatOpenAI  # OpenAI ì±„íŒ… ëª¨ë¸ ì‚¬ìš©, + pip install --upgrade langchain openai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.document_loaders import PyPDFLoader  # PDF ë¬¸ì„œ ë¡œë“œ
from langchain.memory import ConversationBufferMemory  # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬
from langchain.memory.chat_message_histories import StreamlitChatMessageHistory  # Streamlitì—ì„œ ë©”ì‹œì§€ ì €ì¥
from langchain.embeddings import HuggingFaceEmbeddings  # í…ìŠ¤íŠ¸ ì„ë² ë”© (Hugging Face ëª¨ë¸ ì‚¬ìš©)
from langchain.callbacks.base import BaseCallbackHandler  # ì²´ì¸ì˜ ì‘ì—… ì¤‘ ì½œë°± í•¸ë“¤ë§
from langchain.chains import ConversationalRetrievalChain  # ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸
from langchain.vectorstores import DocArrayInMemorySearch  # ë©”ëª¨ë¦¬ ë‚´ ë¬¸ì„œ ê²€ìƒ‰
from langchain.text_splitter import RecursiveCharacterTextSplitter  # ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°



st.set_page_config(page_title="My Cowork ChatBot", page_icon="ğŸ¦œ")  # í˜ì´ì§€ ì œëª©ê³¼ ì•„ì´ì½˜ ì„¤ì •
st.title("LANGCHAIN : My Cowork ChatBot with Documents")  # í˜ì´ì§€ ì œëª© í‘œì‹œ

def configure_retriever(uploaded_files):
    docs = []
    temp_dir = tempfile.TemporaryDirectory()
    
    for file in uploaded_files:
        temp_filepath = os.path.join(temp_dir.name, file.name)
        with open(temp_filepath, "wb") as f:
            f.write(file.getvalue())
        loader = PyPDFLoader(temp_filepath)
        docs.extend(loader.load())
        
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200) 
    splits = text_splitter.split_documents(docs)
    
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2") # all-MiniLM-L6-v2
    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)
    retriever = vectordb.as_retriever(search_type = "mmr", search_kwargs = {"k": 3, "fetch_k": 4})
    # mmr : maximal marginal relevance, ê²€ìƒ‰ì˜ ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„±ì„ ë™ì‹œì— ê³ ë ¤í•´ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ê²€ìƒ‰ì „ëµ
    # k = 2 : ìµœì¢…ì ìœ¼ë¡œ ë°˜í™˜í•  ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆ˜
    # fetch_k : ë‚´ë¶€ì ìœ¼ë¡œ ê²€ìƒ‰í•  ë¬¸ì„œì˜ ìˆ˜ ì„¤ì •.
    # ì´ ê°’ì„ í†µí•´ ê²€ìƒ‰ íš¨ìœ¨ì„±ê³¼ ì •í™•ì„± ê°„ì˜ ê· í˜•ì„ ì¡°ì •.
    
    return retriever

# llm ì‘ë‹µì„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•˜ëŠ” í•¸ë“¤ëŸ¬
# llm ì‹¤í–‰ì‹œì‘ ë° í† í°ìƒì„±ë§ˆë‹¤ ì½œë°±ì„ í˜¸ì¶œí•´ uië¥¼ ì¦‰ê°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•¨
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = ""):
        self.container = container
        self.text = initial_text # ì»¨í…Œì´ë„ˆ ì•ˆì— í‘œê¸°í•  ì´ˆê¸°í…ìŠ¤íŠ¸
        self.run_id_ignore_token = None
        
    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):
        if prompts[0].startswith("Human"): # ì…ë ¥í•œ ì§ˆë¬¸ ì¶œë ¥í•˜ì§€ ì•Šë„ë¡ ìˆ˜í–‰
            self.run_id_ignore_token = kwargs.get("run_id") # ì‹¤í–‰ idë¥¼ ì €ì¥í•´ íŠ¹ì • ì‹¤í–‰ ë¬´ì‹œ
            
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        if self.run_id_ignore_token == kwargs.get("run_id", False):
            return
        self.text += token 
        self.container.markdown(self.text) # streamlit ì»¨í…Œì´ë„ˆì— í…ìŠ¤íŠ¸ ì—…ëƒ
        
class PrintRetrievalHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.status = container.status("**Context Retrieval**")
        
    def on_retriever_start(self, serialized: dict, query : str, **kwargs):
        self.status.write(f"**Question:** {query}") # ì²˜ë¦¬ì¤‘ì¸ ì§ˆë¬¸ í‘œì‹œ
        self.status.update(label = f"**Context Retrieval:** {query}") # ìƒíƒœë ˆì´ë¸” ì—…ëƒ
        
        
    def on_retriever_end(self, documents):
        for idx, doc in enumerate(documents):
            source = os.path.basename(doc.metadata["source"])
            self.status.write(f"**Document {idx} from {source}**")
            self.status.markdown(doc.page_content)
            
        self.status.update(state = "complete") 
        
# 
# api_key = 'AIzaSyDNpp0KaAXNzGQLDTYCopXOKd8lwT7FluI'
# os.environ["GOOGLE_API_KEY"] = api_key 

api_key = st.sidebar.text_input("Google API Key", type="password")

if not api_key:
    st.info("Please add your API key to continue")
    st.stop()
    
uploaded_files = st.sidebar.file_uploader(
    label = "Upload PDF files", type = ["pdf"], accept_multiple_files = True
)

if not uploaded_files:
    st.info("Please upload PDF documents to continue")
    st.stop()
    
# ì—…ë¡œë“œëœ ë¬¸ì„œë¡œ ë¬¸ì„œ ê²€ìƒ‰ê¸° ì„¤ì •
retriever = configure_retriever(uploaded_files)

msgs = StreamlitChatMessageHistory()
# ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš¯ì•  ëŒ€í™”ì˜ ë¬¸ë§¥ì„ ìœ ì§€í•˜ëŠ” í•¨ìˆ˜. ì´ì „ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ ì ì ˆí•œ ì‘ë‹µ ìƒì„±í•˜ë„ë¡ ë„ì™€ì¤Œ
memory = ConversationBufferMemory(memory_key = "chat_history", chat_memory = msgs, return_messages = True)
# return_messages : trueë¼ë©´ ì €ìì™¼ ëŒ€í™” ë‚´ìš©ì„ ë©”ì‹œì§€ ê°ì²´ë¡œ ë°˜í™˜

llm = ChatGoogleGenerativeAI(model = "gemini-1.5-flash", temperature = 0.05, google_api_key = api_key)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm, retriever=retriever, memory=memory, verbose=True
) 

if len(msgs.messages) == 0 or st.sidebar.button("Clear message history"):
    msgs.clear()
    msgs.add_ai_message("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?") # ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€
    
# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì— í•´ë‹¹ ì•„ë°”íƒ€ë¡œ ë©”ì‹œì§€ í‘œì‹œí•˜ë„ë¡
avatars = {"human" : "user", "ai": "assistant"}

# ë©”ì‹œì§€ ê¸°ë¡ ë°˜ë³µí•˜ë©° í•´ë‹¹ ì•„ë°”íƒ€ë¡œ ë©”ì‹œì§€ í‘œì‹œ
for msg in msgs.messages:
    st.chat_message(avatars[msg.type]).write(msg.content)
    
if user_query := st.chat_input(placeholder = "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”!"): # ì‚¬ìš©ìì… ì…ë ¥ëŒ€ê¸°
    st.chat_message("user").write(user_query) # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
    
    with st.chat_message("assistant"):
        retrieval_handler = PrintRetrievalHandler(st.container())
        stream_handler = StreamHandler(st.empty())
        response = qa_chain.run(user_query, callbacks = [retrieval_handler, stream_handler]) 